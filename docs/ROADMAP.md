# Project Roadmap

This roadmap outlines the full development lifecycle for a distributed key-value
store written in Rust. The goal is to create a production-ready, horizontally
scalable, self-hosted system that supports data replication, consensus,
persistence, and observability. It will be Docker-deployable and suitable for
real-world deployment in small to medium-scale environments.

---

## ğŸ§± Phase 1: Single-Node Foundation

### ğŸ¯ Objectives

- Implement core storage engine (in-memory + disk-based)
- Support persistence through a Write-Ahead Log (WAL)
- Provide a simple API to interact with the node

### ğŸ“‹ Features

- `SET`, `GET`, `DELETE` operations
- In-memory store backed by `HashMap`
- Append-only Write-Ahead Log (WAL)
- WAL replay on startup
- Basic HTTP interface
- Simple configuration file
- Graceful shutdown and error handling
- Pluggable logging system

### ğŸ›  Tasks
- Define `StorageEngine` trait and default `MemoryEngine`
- Implement WAL format (e.g., bincode or protobuf)
- Create replay mechanism for recovery
- Build HTTP API
- Integrate logging
- Unit tests and integration tests
- Benchmarks

---

## ğŸ—ƒï¸ Phase 2: Persistent Storage Abstraction

### ğŸ¯ Objectives

- Modularize and enhance persistence
- Allow flexible storage backends (e.g., RocksDB, SSTables)

### ğŸ“‹ Features

- Pluggable persistence engine
- WAL compaction
- Snapshotting mechanism

### ğŸ›  Tasks

- Abstract WAL and flushable storage backend
- Add log compaction and snapshot support
- Implement `RocksDbEngine` as optional backend
- Add benchmark suite to compare engines

## ğŸŒ Phase 3: Networking & Cluster Awareness

### ğŸ¯ Objectives

- Enable communication between nodes
- Build cluster membership and topology awareness

### ğŸ“‹ Features

- Unique node identity
- TCP/gRPC server on each node
- Cluster join/leave protocol
- Discovery via gossip, DNS, or static config
    - Probably starting with static config in this phase...
- Cluster metadata service

### ğŸ›  Tasks

- Define message format (protobuf)
- Implement gRPC or raw TCP messaging
- Build membership service with periodic heartbeats
- Create gossip-based cluster state propogator
- Add node health monitoring and state transitions

## âš–ï¸ Phase 4: Sharding & Key Distribution

### ğŸ¯ Objectives

- Distribute keys across multiple nodes
- Support cluster scaling with data rebalancing

### ğŸ“‹ Features

- Consistent hash ring
- Shard allocation
- Dynamic node join/leave with rebalancing
- Node-specific shard maps

### ğŸ›  Tasks

- Implement consistent hashing ring
- Map key ranges to shard responsibilities
- Create shard assignment module
- Implement data migration and balancing logic

## ğŸ” Phase 5: Replication & Consensus

### ğŸ¯ Objectives

- Ensure fault tolerance via replication
- Maintain consistency using Raft protocol

### ğŸ“‹ Features

- Configurable replication factor (e.g., RF=3)
- Raft-based leader election and log replication
- Follower recovery and state sync
- Read and write quorum enforcement

### ğŸ›  Tasks

- Integrate Raft (try to implement own version of algorithm)
- Implement Raft state machine for log commits
- Design data replication flow (WAL replication, snapshots)
- Implement catch-up for out-of-sync followers
- Add failure detection and retry mechanisms

## ğŸ›¡ï¸ Phase 6: Security & Access Control

### ğŸ¯ Objectives

- Protect system access and node communication

### ğŸ“‹ Features

- TLS encryption for all gRPC/TCP communication
- Basic Auth or JWT for API access
- Role-based Access Control (RBAC)

### ğŸ›  Tasks

- Enable mutual TLS between nodes
- Secure admin APIs with JWT tokens
- Design and enforce access control rules
- Integrate optional authentication layer

## ğŸ§­ Phase 7: Routing Layer & Load Balancer

### ğŸ¯ Objectives

- Build a gateway or router node for external access
- Support autoscaling and routing to correct nodes

### ğŸ“‹ Features

- Client-facing router service
- Request routing based on consistent hash
- Health checks and circuit breakers
- Retry logic and failover routing

### ğŸ›  Tasks

- Build stateless router binary
- Integrate cluster membership awareness
- Implement key-based routing logic
- Handle request deduplication and timeouts

## ğŸ“¦ Phase 8: Deployment & Dockerization

### ğŸ¯ Objectives

- Provide an easy-to-deploy solution using Docker

### ğŸ“‹ Features

- Dockerfiles for all components
- Docker Compose for development
- Helm chart or K8s manifests

### ğŸ›  Tasks

- Write Dockerfiles for core storage server and router applications
- Add volume mounting for persistent WAL data
- Compose file with 3-node cluster + router
- Write deployment guide and config examples

## ğŸ“Š Phase 9: Observability & Monitoring

### ğŸ¯ Objectives

- Make system observable and debuggable

### ğŸ“‹ Features

- Structured logs
- Prometheus metrics endpoint
- Distributed tracing
- Metrics for WAL, storage, Raft state, RPC latency

### ğŸ›  Tasks

- Add `metrics` or `prometheus` crate
- Instrument all subsystems (WAL, Raft, Router)
- Expose `/metrics` endpoint on all components
- Export logs in structured JSON format

## ğŸš€ Future

- Distributed transactions
- TTL for keys and automatic eviction
- Web dashboard for observability and management
- Backup and restore support
- Pub/sub and streaming capabilities
- API client SDKs
- Multi-datacenter support

